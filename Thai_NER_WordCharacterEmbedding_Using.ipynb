{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thai Named Entity Recognition Using Bi-LSTM + CRF\n",
    "## The model use Bi-directional LSTM with Word / Character representation and CRF for sequece tagging\n",
    "\n",
    "perform keras backend\n",
    "\n",
    "- Word Embedding : 0.32 Thai2Fit 400 dimension\n",
    "- Char Embedding : LSTM training from scratch 32 dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare Path for Model and Dataset\n",
    "Declare user path to load/save model and dataset\n",
    "In this file include:\n",
    "- raw path\n",
    "- model path\n",
    "- word embedding path\n",
    "- dictionary path (char2index / ner2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH='./'\n",
    "RAW_PATH = f'{DATA_PATH}raw/'\n",
    "MODEL_PATH = f'{DATA_PATH}model/Keras/WordCharModel/'\n",
    "W_MODEL_PATH = f'{DATA_PATH}model/thai2fit/'\n",
    "Dict_MODEL_PATH = f'{DATA_PATH}model/dictionary/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Version\n",
    "\n",
    "Keras 2.1.6 (pip install keras==2.1.6)\n",
    "\n",
    "Python 3.5.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.6\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "# Save / Load File\n",
    "import dill\n",
    "import pickle\n",
    "\n",
    "# Plot Graph\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sklearn Report\n",
    "import sklearn\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from itertools import chain\n",
    "\n",
    "# Load Vectors\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Utility\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Model Utility\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# Keras Model\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model, Input\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Conv1D\n",
    "from keras.layers import Bidirectional, concatenate, SpatialDropout1D, GlobalMaxPooling1D\n",
    "from keras_contrib.layers import CRF\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "\n",
    "# ELMO Model\n",
    "from allennlp.modules.elmo import Elmo, batch_to_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Thai2Fit Word Embedding\n",
    "load Binary file of thai2fit (0.32) train wikipedia using ULMFit model\n",
    "\n",
    "the word vector size is 55677x400 dimensions\n",
    "\n",
    "credit: https://github.com/cstorm125/thai2fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "thai2fit_model = KeyedVectors.load_word2vec_format(W_MODEL_PATH+'thai2vecNoSym.bin',binary=True)\n",
    "thai2fit_weight = thai2fit_model.vectors\n",
    "\n",
    "thai2dict = {}\n",
    "\n",
    "for word in thai2fit_model.index2word:\n",
    "    thai2dict[word] = thai2fit_model[word]\n",
    "\n",
    "all_thai2dict = sorted(set(thai2dict))\n",
    "thai2dict_to_ix = dict((c, i) for i, c in enumerate(thai2dict)) #convert thai2fit to index \n",
    "ix_to_thai2dict = dict((v,k) for k,v in thai2dict_to_ix.items())  #convert index to thai2fit\n",
    "\n",
    "n_thai2dict = len(thai2dict_to_ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load NER Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Dict_MODEL_PATH+'nerdict.pickle', 'rb') as nerdict:\n",
    "    ner_to_ix = pickle.load(nerdict)\n",
    "\n",
    "ix_to_ner = dict((v,k) for k,v in ner_to_ix.items())  #convert index to ner\n",
    "n_tag = len(ner_to_ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Character Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Dict_MODEL_PATH+'chardict.pickle', 'rb') as chardict:\n",
    "    char2idx = pickle.load(chardict)\n",
    "\n",
    "n_chars = len(char2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Parameter and Hyper Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 250\n",
    "max_len_char = 30\n",
    "\n",
    "character_LSTM_unit = 32\n",
    "char_embedding_dim = 32\n",
    "main_lstm_unit = 256 ## Bidirectional 256 + 256 = 512\n",
    "lstm_recurrent_dropout = 0.5\n",
    "\n",
    "train_batch_size = 32\n",
    "train_epochs = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence_word(input_text):\n",
    "    idxs = list()\n",
    "    for word in input_text:\n",
    "        if word in thai2dict:\n",
    "            idxs.append(thai2dict_to_ix[word])\n",
    "        else:\n",
    "            idxs.append(thai2dict_to_ix[\"unknown\"]) #Use UNK tag for unknown word\n",
    "    return idxs\n",
    "\n",
    "def prepare_sequence_target(input_label):\n",
    "    idxs = [ner_to_ix[w] for w in input_label]\n",
    "    return idxs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/suphanut_thattinaphanich/anaconda3/envs/ulmfit_venv/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/suphanut_thattinaphanich/anaconda3/envs/ulmfit_venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3144: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/suphanut_thattinaphanich/anaconda3/envs/ulmfit_venv/lib/python3.6/site-packages/keras_contrib/layers/crf.py:346: UserWarning: CRF.loss_function is deprecated and it might be removed in the future. Please use losses.crf_loss instead.\n",
      "  warnings.warn('CRF.loss_function is deprecated '\n",
      "/home/suphanut_thattinaphanich/anaconda3/envs/ulmfit_venv/lib/python3.6/site-packages/keras_contrib/layers/crf.py:353: UserWarning: CRF.accuracy is deprecated and it might be removed in the future. Please use metrics.crf_accuracy\n",
      "  warnings.warn('CRF.accuracy is deprecated and it '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "char_input (InputLayer)         (None, 250, 30)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "word_input_ (InputLayer)        (None, 250)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 250, 30, 32)  12768       char_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "word_embedding (Embedding)      (None, 250, 400)     22270800    word_input_[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, 250, 64)      16640       time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 250, 464)     0           word_embedding[0][0]             \n",
      "                                                                 time_distributed_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_1 (SpatialDro (None, 250, 464)     0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 250, 512)     1476608     spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_3 (TimeDistrib (None, 250, 50)      25650       bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "crf_1 (CRF)                     (None, 250, 27)      2160        time_distributed_3[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 23,804,626\n",
      "Trainable params: 1,533,826\n",
      "Non-trainable params: 22,270,800\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Word Input\n",
    "word_in = Input(shape=(max_len,), name='word_input_')\n",
    "\n",
    "# Word Embedding Using Thai2Fit\n",
    "word_embeddings = Embedding(input_dim=n_thai2dict,\n",
    "                            output_dim=400,\n",
    "                            weights = [thai2fit_weight],input_length=max_len,\n",
    "                            mask_zero=False,\n",
    "                            name='word_embedding', trainable=False)(word_in)\n",
    "\n",
    "# Character Input\n",
    "char_in = Input(shape=(max_len, max_len_char,), name='char_input')\n",
    "\n",
    "# Character Embedding\n",
    "emb_char = TimeDistributed(Embedding(input_dim=n_chars, output_dim=char_embedding_dim, \n",
    "                           input_length=max_len_char, mask_zero=False))(char_in)\n",
    "\n",
    "# Character Sequence to Vector via BiLSTM\n",
    "char_enc = TimeDistributed(Bidirectional(LSTM(units=character_LSTM_unit, return_sequences=False, recurrent_dropout=lstm_recurrent_dropout)))(emb_char)\n",
    "\n",
    "\n",
    "# Concatenate All Embedding\n",
    "all_word_embeddings = concatenate([word_embeddings, char_enc])\n",
    "all_word_embeddings = SpatialDropout1D(0.3)(all_word_embeddings)\n",
    "\n",
    "# Main Model BiLSTM\n",
    "main_lstm = Bidirectional(LSTM(units=main_lstm_unit, return_sequences=True,\n",
    "                               recurrent_dropout=lstm_recurrent_dropout))(all_word_embeddings)\n",
    "main_lstm = TimeDistributed(Dense(50, activation=\"relu\"))(main_lstm)\n",
    "\n",
    "# CRF\n",
    "crf = CRF(n_tag)  # CRF layer\n",
    "out = crf(main_lstm)  # output\n",
    "\n",
    "# Model\n",
    "model = Model([word_in, char_in], out)\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=crf.loss_function, metrics=[crf.accuracy])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Weight Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_filepath=MODEL_PATH+\"weights-improvement-46-0.996.hdf5\"\n",
    "model.load_weights(load_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_word_to_char(predict_word):\n",
    "    predict_char = []\n",
    "    sent_seq = []\n",
    "    for i in range(max_len):\n",
    "        word_seq = []\n",
    "        for j in range(max_len_char):    \n",
    "            try:\n",
    "                if(predict_word[i][j] in char2idx):\n",
    "                    word_seq.append(char2idx.get(predict_word[i][j]))\n",
    "                else:\n",
    "                    word_seq.append(char2idx.get(\"unknown\"))\n",
    "            except:\n",
    "                word_seq.append(char2idx.get(\"pad\"))\n",
    "        sent_seq.append(word_seq)\n",
    "    predict_char.append(np.array(sent_seq))\n",
    "    \n",
    "    return predict_char"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Input for prediction\n",
    "\n",
    "tokenization Thai word -> generate word list + padding -> generate char list + padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"นายธนาธรเจอนางสาวยิ่งลักษ์ที่มหาวิทยาลัยจุฬา เช้าวันนี้\"\n",
    "\n",
    "predict_sent = word_tokenize(text,engine='newmm')\n",
    "len_word = len(predict_sent)\n",
    "\n",
    "predict_word = []\n",
    "predict_word = [prepare_sequence_word(predict_sent)]\n",
    "predict_word = pad_sequences(maxlen=max_len, sequences=predict_word, value=thai2dict_to_ix[\"pad\"], padding='post', truncating='post')\n",
    "\n",
    "predict_char = convert_word_to_char(predict_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_tag = model.predict([predict_word,np.array(predict_char).reshape((len(predict_char),max_len, max_len_char))])\n",
    "p = np.argmax(result_tag, axis=-1)\n",
    "pred=[i for i in p[0]]\n",
    "revert_pred=[ix_to_ner[i] for i in p[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['นาย', 'ธนา', 'ธร', 'เจอ', 'นางสาว', 'ยิ่ง', 'ลัก', 'ษ์', 'ที่', 'มหาวิทยาลัย', 'จุฬา', ' ', 'เช้า', 'วันนี้']\n",
      "[8, 21, 21, 25, 8, 21, 21, 21, 25, 6, 19, 25, 10, 0]\n",
      "['B-PERSON', 'I-PERSON', 'I-PERSON', 'O', 'B-PERSON', 'I-PERSON', 'I-PERSON', 'I-PERSON', 'O', 'B-ORGANIZATION', 'I-ORGANIZATION', 'O', 'B-TIME', 'B-DATE']\n"
     ]
    }
   ],
   "source": [
    "print(predict_sent)\n",
    "print(pred[:len_word])\n",
    "print(revert_pred[:len_word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
